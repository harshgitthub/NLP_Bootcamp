{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import nltk\n"
      ],
      "metadata": {
        "id": "eogmQ6eHzxpj"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lower and upper casing\n",
        "text = \"This is About the morning to cOME And i AM doinG this AssignemnT\"\n",
        "lowercased_text = text.lower()\n",
        "print(lowercased_text)\n",
        "uppercased_text = text.upper()\n",
        "print(uppercased_text)"
      ],
      "metadata": {
        "id": "i21tiq4Fz1Vn",
        "outputId": "9a876d5b-0536-4b26-f897-84228ea8eef5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "this is about the morning to come and i am doing this assignemnt\n",
            "THIS IS ABOUT THE MORNING TO COME AND I AM DOING THIS ASSIGNEMNT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing stop words from a corpus\n",
        "# also showing tokenizartion\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "text = 'A corpus donation is a donation that comes with a specific written direction from the donor to the recipient organization.The donation is then used to create a corpus fund, which is a permanent financial reserve for the organization.'\n",
        "# removing stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "word_tokens = word_tokenize(text) # can also do text.split() here\n",
        "filtered_text = [word for word in word_tokens if word not in stop_words]\n",
        "filtered_text\n"
      ],
      "metadata": {
        "id": "Ny0nxoCe0D7E",
        "outputId": "38332d31-b809-4017-d03b-f25f957c15e4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['A',\n",
              " 'corpus',\n",
              " 'donation',\n",
              " 'donation',\n",
              " 'comes',\n",
              " 'specific',\n",
              " 'written',\n",
              " 'direction',\n",
              " 'donor',\n",
              " 'recipient',\n",
              " 'organization.The',\n",
              " 'donation',\n",
              " 'used',\n",
              " 'create',\n",
              " 'corpus',\n",
              " 'fund',\n",
              " ',',\n",
              " 'permanent',\n",
              " 'financial',\n",
              " 'reserve',\n",
              " 'organization',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# stemming\n",
        "# 1. Porter stemmer\n",
        "\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "words = [\"running\", \"jumps\", \"easily\", \"fairly\", \"happiness\",\"joyful\", \"flying\", \"cats\", \"studies\", \"studying\"]\n",
        "\n",
        "# stemming removes common suffixes from the end of the words\n",
        "\n",
        "print(\"Porter stemmer\")\n",
        "stemmed_words = [stemmer.stem(word) for word in words]\n",
        "stemmed_words\n"
      ],
      "metadata": {
        "id": "zYZPnrYT0_HD",
        "outputId": "460d4ffa-07b1-4703-8bac-4842e7884b16",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['run',\n",
              " 'jump',\n",
              " 'easili',\n",
              " 'fairli',\n",
              " 'happi',\n",
              " 'joy',\n",
              " 'fli',\n",
              " 'cat',\n",
              " 'studi',\n",
              " 'studi']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "\n",
        "words = [\"running\", \"jumps\", \"easily\", \"fairly\", \"happiness\",\"joyful\", \"flying\", \"cats\", \"studies\", \"studying\"]\n",
        "\n",
        "# lemmatizing converts the word into the dictionary form\n",
        "\n",
        "print(\"WordNet Lemmatizer\")\n",
        "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
        "lemmatized_words"
      ],
      "metadata": {
        "id": "awQkya5W17u_",
        "outputId": "e4b53d5e-6a06-4759-c6b3-342507fd45f6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WordNet Lemmatizer\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['running',\n",
              " 'jump',\n",
              " 'easily',\n",
              " 'fairly',\n",
              " 'happiness',\n",
              " 'joyful',\n",
              " 'flying',\n",
              " 'cat',\n",
              " 'study',\n",
              " 'studying']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Showing the use of regex **  "
      ],
      "metadata": {
        "id": "qgMeJme62gO9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# showing the use of regex to remove\n",
        "import re\n",
        "\n",
        "# 1. URl's\n",
        "def remove_urls(text):\n",
        "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    return url_pattern.sub(r'', text)\n",
        "\n",
        "text = \"this is a hacking working which i am doing at 4 in the morning https://example.com\"\n",
        "text_without_urls = remove_urls(text)\n",
        "print(text_without_urls)\n",
        "\n",
        "\n",
        "#2. Html tags\n",
        "\n",
        "text = \"\"\"<html><div>\n",
        "<h1>Me-Doing hacking</h1>\n",
        "<p>will hack one day ASC</p>\n",
        "<a href=\"https://kaliLinux.com\"></a>\n",
        "</div></html>\"\"\"\n",
        "\n",
        "html_tags_pattern = r'<.*?>'\n",
        "text_without_html_tags = re.sub(html_tags_pattern, '', text)\n",
        "print(text_without_html_tags)\n",
        "\n",
        "# 3. punctuation removal\n",
        "text = \"Hello, checkers âœ‹âœ‹! Welcome to?* my&/|~^+%'\\\" NLP - Deep LearningðŸ§  submissionðŸ¤©.\"\n",
        "punctuation_pattern = r'[^\\w\\s]'\n",
        "text_cleaned = re.sub(punctuation_pattern, '', text)\n",
        "print(text_cleaned)"
      ],
      "metadata": {
        "id": "PYYRR1k32f82",
        "outputId": "b74c3cae-7642-4786-a0bc-990244f17bcf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "this is a hacking working which i am doing at 4 in the morning \n",
            "\n",
            "Me-Doing hacking\n",
            "will hack one day ASC\n",
            "\n",
            "\n",
            "Hello checkers  Welcome to my NLP  Deep Learning submission\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}