{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Assignment 2**\n",
    "## **Artificial Neural Network for Text Sentiment Classification**\n",
    "\n",
    "Text sentiment classification using neural networks (NN) involves training a model to analyze and classify the sentiment expressed in text, such as positive, negative, or neutral. Neural networks, particularly recurrent networks (RNNs) or transformers (which will be explored subsequently), can capture the sequential nature of text data, making them well-suited for this task. By embedding words into dense vectors, a feed forward neural network can learn to associate patterns in word usage with sentiment labels. During training, the model adjusts its weights to minimize error, enabling it to generalize to unseen text, ultimately predicting sentiment with high accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "===================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "blue {\n",
    "  color: skyblue;\n",
    "}\n",
    "\n",
    "green {\n",
    "  color: lightgreen;\n",
    "}\n",
    "</style>\n",
    "\n",
    "### **Step - 1**\n",
    "This code imports necessary libraries for text preprocessing, model building, and evaluation.\n",
    "1)  It uses <blue>**pandas**</blue> for <green>**data handling**</green>. \n",
    "2) <blue>**Matplotlib**</blue> for <green>**plotting**</green>. \n",
    "3) <blue>**NLTK**</blue> for <green>**tokenizing**</green> and <green>**stemming**</green> text. \n",
    "4) It includes <blue>**scikit-learn**</blue> for <green>**splitting data**</green> and <green>**evaluation (classification_report)**</green>.\n",
    "5) <blue>**Torch**</blue> is used to <green>**define**</green>, <green>**train**</green>, and <green>**optimize**</green> neural networks. \n",
    "6) <blue>**gensim**</blue> is used for <green>**dictionary creation**</green> and <green>**token mapping**</green>, helping to convert text into numerical format for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "from gensim import corpora\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "blue {\n",
    "  color: skyblue;\n",
    "}\n",
    "\n",
    "green {\n",
    "  color: lightgreen;\n",
    "}\n",
    "</style>\n",
    "\n",
    "### **Step - 2**\n",
    "This code <green>**loads**</green> the <blue>**\"yelp_reviews_subset_2.csv\"**</blue> dataset downloaded from the Notion page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Download and Load the \"yelp_reviews_subset_2.csv\" dataset uploaded on the notion page using pandas\n",
    "df = None # Replace None with the correct code\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "blue {\n",
    "  color: skyblue;\n",
    "}\n",
    "\n",
    "green {\n",
    "  color: lightgreen;\n",
    "}\n",
    "</style>\n",
    "\n",
    "### **Step - 3**\n",
    "This code maps review star ratings to sentiment categories and visualizes the sentiment distribution.\n",
    "\n",
    "1) The function <blue>**map_sentiment**</blue> converts star ratings into <green>**three categories**</green>: negative (-1 for stars ≤ 2), neutral (0 for 3 stars), and positive (1 for stars ≥ 4).\n",
    "2) It applies this function to the <blue>**Labels**</blue> column in df, creating a new column <blue>**sentiment**</blue>.\n",
    "3) The code then plots a bar chart showing the <green>**distribution of the sentiments**</green> using matplotlib and pandas, labeling the x-axis as \"Sentiment\" and the y-axis as \"No. of rows in df\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_sentiment(stars_received):\n",
    "    if stars_received <= 2:\n",
    "        return -1\n",
    "    elif stars_received == 3:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "    \n",
    "# Mapping stars to sentiment into three categories\n",
    "df['sentiment'] = [ map_sentiment(x) for x in df['Labels']]\n",
    "# Plotting the sentiment distribution\n",
    "plt.figure()\n",
    "pd.value_counts(df['sentiment']).plot.bar(title=\"Sentiment distribution in df\")\n",
    "plt.xlabel(\"Sentiment\")\n",
    "plt.ylabel(\"No. of rows in df\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "blue {\n",
    "  color: skyblue;\n",
    "}\n",
    "\n",
    "green {\n",
    "  color: lightgreen;\n",
    "}\n",
    "</style>\n",
    "\n",
    "### **Step - 4**\n",
    "This code tokenizes text and stems the words using NLTK tools.\n",
    "\n",
    "1) It <green>**tokenizes**</green> each sentence in the dataframe <blue>**df['tokenized_text']**</blue> using <blue>**word_tokenize**</blue> from NLTK, applying <green>**list comprehension**</green> to store the tokens as a list.\n",
    "2) Next, it uses the <blue>**Porter Stemmer**</blue> to <green>**stem**</green> each token in <blue>**df['tokenized_text']**</blue>, storing the stemmed words in <blue>**df['stemmed_tokens']**</blue> using <green>**list comprehension**</green>.\n",
    "\n",
    "Both processes are applied to make the text more suitable for sentiment classification by reducing it to basic word forms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Tokenize each sentence into word tokens and store them as a list in the dataframe\n",
    "# Use List Comprehension\n",
    "df['tokenized_text'] = None # Replace None with the correct code \n",
    "print(df['tokenized_text'].head(10))\n",
    "\n",
    "ps = PorterStemmer()\n",
    "# TODO: Use the Porter Stemmer to find stem words of each word for all the words in df[\"tokenized_text\"]\n",
    "# Hint: Use List Comprehension\n",
    "df['stemmed_tokens'] = None # Replace None with the correct code\n",
    "df['stemmed_tokens'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "blue {\n",
    "  color: skyblue;\n",
    "}\n",
    "\n",
    "green {\n",
    "  color: lightgreen;\n",
    "}\n",
    "</style>\n",
    "\n",
    "### **Step - 5**\n",
    "This code splits the dataset into training and testing sets for model evaluation.\n",
    "<blue>**split_train_test**</blue> function uses <blue>**train_test_split**</blue> from scikit-learn to divide the dataset into <green>**training (70%)**</green> and <green>**testing (30%)**</green> sets based on the tokenized_text, stemmed_tokens, and other columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Test Split Function\n",
    "def split_train_test(df, test_size=0.3, shuffle_state=True):\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(df[[\"Text\", \"Labels\", \"tokenized_text\", \"stemmed_tokens\"]], \n",
    "                                                        df['sentiment'], \n",
    "                                                        shuffle=shuffle_state,\n",
    "                                                        test_size=test_size, \n",
    "                                                        random_state=15)\n",
    "    print(\"Value counts for Train sentiments\")\n",
    "    print(Y_train.value_counts())\n",
    "    print(\"Value counts for Test sentiments\")\n",
    "    print(Y_test.value_counts())\n",
    "    print(type(X_train))\n",
    "    print(type(Y_train))\n",
    "    X_train = X_train.reset_index()\n",
    "    X_test = X_test.reset_index()\n",
    "    Y_train = Y_train.to_frame()\n",
    "    Y_train = Y_train.reset_index()\n",
    "    Y_test = Y_test.to_frame()\n",
    "    Y_test = Y_test.reset_index()\n",
    "    print(X_train.head())\n",
    "    return X_train, X_test, Y_train, Y_test\n",
    "\n",
    "# Call the train_test_split\n",
    "X_train, X_test, Y_train, Y_test = split_train_test(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "blue {\n",
    "  color: skyblue;\n",
    "}\n",
    "\n",
    "green {\n",
    "  color: lightgreen;\n",
    "}\n",
    "</style>\n",
    "\n",
    "### **Step - 6**\n",
    "This code sets the <blue>**device**</blue> for running the model, either using <green>**GPU**</green> if available or falling back to <green>**CPU**</green>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Set device (GPU if available, else CPU)\n",
    "device = None # Replace None with the correct code \n",
    "print(\"Device available for running: \")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "blue {\n",
    "  color: skyblue;\n",
    "}\n",
    "\n",
    "green {\n",
    "  color: lightgreen;\n",
    "}\n",
    "</style>\n",
    "\n",
    "### **Step - 7**\n",
    "This code defines a Feedforward Neural Network model with three fully connected layers using PyTorch.\n",
    "\n",
    "1) <blue>**fc1**</blue>: First fully connected layer that takes the <green>**input dimension (input_dim)**</green> and maps it to a <green>**hidden dimension (hidden_dim)**</green>.\n",
    "2) <blue>**fc2**</blue>: Second fully connected layer that maps the <green>**hidden dimension (hidden_dim)**</green> to another <green>**hidden dimension (hidden_dim)**</green>.\n",
    "3) <blue>**fc3**</blue>: Final layer that maps the <green>**hidden dimension**</green> to the <green>**output dimension (output_dim)**</green>, which corresponds to the sentiment classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedforwardNeuralNetModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(FeedforwardNeuralNetModel, self).__init__()\n",
    "        \n",
    "        # TODO: Add an fc1 layer as described below\n",
    "        # Linear function 1: vocab_size --> hiiden_dim\n",
    "        self.fc1 = None # Replace None with the correct code nn.Linear(input_size+hidden_size , output_size)\n",
    "        \n",
    "        # Non-linearity 1\n",
    "        self.relu1 = nn.ReLU()\n",
    "\n",
    "        # TODO: Add an fc2 layer as described below\n",
    "        # Linear function 2: hidden_dim --> hidden_dim\n",
    "        self.fc2 = None # Replace None with the correct code\n",
    "        \n",
    "        # Non-linearity 2\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "        # TODO: Add an fc3 layer as described below\n",
    "        # Linear function 3 (readout): hidden_dim --> output_dim\n",
    "        self.fc3 = None  # Replace None with the correct code\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # TODO: Forward pass through fully connected layer 1\n",
    "        out = None # Replace None with the correct code\n",
    "\n",
    "        # Non-linearity 1\n",
    "        out = self.relu1(out)\n",
    "\n",
    "        # TODO: Forward pass through fully connected layer 2\n",
    "        out = None # Replace None with the correct code\n",
    "        \n",
    "        # Non-linearity 2\n",
    "        out = self.relu2(out)\n",
    "\n",
    "        # TODO: Forward pass through fully connected layer 3\n",
    "        out = None # Replace None with the correct code\n",
    "\n",
    "        return F.softmax(out, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "blue {\n",
    "  color: skyblue;\n",
    "}\n",
    "\n",
    "green {\n",
    "  color: lightgreen;\n",
    "}\n",
    "</style>\n",
    "\n",
    "### **Step - 8**\n",
    "This code defines a function to create a dictionary of tokens from a DataFrame using Gensim's corpora.Dictionary.\n",
    "\n",
    "1) <blue>**make_dict**</blue> generates a <green>**token dictionary**</green> from the <blue>**stemmed_tokens**</blue> column in the DataFrame.\n",
    "2) If <blue>**padding=True**</blue>, it adds a <green>**padding token ('pad')**</green> to the dictionary, which is useful for models that require fixed input sizes.\n",
    "3) If <blue>**padding=False**</blue>, the dictionary is created directly from the tokenized text <green>**without adding a padding token**</green>.\n",
    "4) The <blue>**review_dict**</blue> variable stores the dictionary, and in this case, it is created <green>**without padding**</green> by calling the function with <blue>**padding=False**</blue>.\n",
    "\n",
    "This process is helpful for converting text tokens into numerical indices that can be used as input for machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to return the dictionary either with padding word or without padding\n",
    "def make_dict(top_data_df_small, padding=True):\n",
    "    if padding:\n",
    "        print(\"Dictionary with padded token added\")\n",
    "        review_dict = corpora.Dictionary([['pad']])\n",
    "        review_dict.add_documents(top_data_df_small['stemmed_tokens'])\n",
    "    else:\n",
    "        print(\"Dictionary without padding\")\n",
    "        review_dict = corpora.Dictionary(top_data_df_small['stemmed_tokens'])\n",
    "    return review_dict\n",
    "\n",
    "# Make the dictionary without padding for the basic models\n",
    "review_dict = make_dict(df, padding=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "blue {\n",
    "  color: skyblue;\n",
    "}\n",
    "\n",
    "green {\n",
    "  color: lightgreen;\n",
    "}\n",
    "</style>\n",
    "\n",
    "### **Step - 9**\n",
    "This code creates functions to generate input and target tensors for the neural network.\n",
    "\n",
    "1) <blue>**make_bow_vector**</blue>: This function creates a <green>**bag-of-words (BOW)**</green> vector from a tokenized sentence.\n",
    "\n",
    "2) It initializes a <blue>**zero vector**</blue> of size <blue>**VOCAB_SIZE (30,056)**</blue>, where <green>**each index**</green> represents a word from the vocabulary.\n",
    "\n",
    "3) <blue>**make_target**</blue>: This function converts a <green>**sentiment label (-1, 0, or 1)**</green> into a tensor for the output.\n",
    "\n",
    "4) <blue>**Negative sentiment (-1)**</blue> maps to <green>**0**</green>, <blue>**neutral sentiment (0)**</blue> maps to <green>**1**</green>, and <blue>**positive sentiment (1)**</blue> maps to <green>**2**</green>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 30056\n",
    "NUM_LABELS = 3\n",
    "\n",
    "# Function to make bow vector to be used as input to network\n",
    "def make_bow_vector(review_dict, sentence):\n",
    "    vec = torch.zeros(VOCAB_SIZE, dtype=torch.float64, device=device)\n",
    "    for word in sentence:\n",
    "        vec[review_dict.token2id[word]] += 1\n",
    "    return vec.view(1, -1).float()\n",
    "\n",
    "# Function to get the output tensor\n",
    "def make_target(label):\n",
    "    if label == -1:\n",
    "        return torch.tensor([0], dtype=torch.long, device=device)\n",
    "    elif label == 0:\n",
    "        return torch.tensor([1], dtype=torch.long, device=device)\n",
    "    else:\n",
    "        return torch.tensor([2], dtype=torch.long, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "blue {\n",
    "  color: skyblue;\n",
    "}\n",
    "\n",
    "green {\n",
    "  color: lightgreen;\n",
    "}\n",
    "</style>\n",
    "\n",
    "### **Step - 10**\n",
    "1) <blue>**Model Initialization**</blue>: The <green>**Feedforward Neural Network**</green> is instantiated with input_dim, hidden_dim, and output_dim.\n",
    "2) <blue>**Device Transfer**</blue>: The model is moved to the <green>**GPU**</green> or <green>**CPU**</green> as specified earlier.\n",
    "3) <blue>**Loss Function**</blue>: The loss function is set to <green>**CrossEntropyLoss**</green>, suitable for multi-class classification.\n",
    "4) <blue>**Optimizer**</blue>: The optimizer is <green>**SGD (Stochastic Gradient Descent)**</green> with a <green>**learning rate**</green> of <green>**1e-3**</green>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(review_dict)\n",
    "\n",
    "input_dim = VOCAB_SIZE\n",
    "hidden_dim = 500\n",
    "output_dim = 3\n",
    "num_epochs = 100\n",
    "\n",
    "# TODO: Call the FeedForwardNeuralNetwork Class object with appropriate arguements\n",
    "ff_nn_bow_model = None # Replace None with the correct code\n",
    "\n",
    "# TODO: Move the model to device\n",
    "ff_nn_bow_model = None # Replace None with the correct code\n",
    "\n",
    "# TODO: Define the loss function as Cross Entropy Loss\n",
    "loss_function = None # Replace None with the correct code\n",
    "\n",
    "# TODO: Define a Stochastic Gradient Descent Optimizer with learning rate of 1e-3\n",
    "optimizer = None # Replace None with the correct code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "blue {\n",
    "  color: skyblue;\n",
    "}\n",
    "\n",
    "green {\n",
    "  color: lightgreen;\n",
    "}\n",
    "</style>\n",
    "\n",
    "### **Step - 11**\n",
    "1) <blue>**Bag-of-Words Vector**</blue>: The <green>**make_bow_vector**</green> function is used to create a BOW vector from the stemmed tokens.\n",
    "2) <blue>**Forward Pass**</blue>: The <green>**BOW vector**</green> is passed through the <green>**feedforward neural network**</green> to get the output probabilities.\n",
    "3) <blue>**Loss Calculation**</blue>: The <green>**CrossEntropyLoss**</green> is computed between the output probabilities and the target label.\n",
    "4) <blue>**Gradient Update**</blue>: <green>**Gradients**</green> are calculated and the parameters are updated using backpropagation and <green>**optimizer.step()**</green>.\n",
    "5) <blue>**Loss Logging**</blue>: The <green>**average loss per epoch**</green> is written to the loss file for tracking training progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the file for writing loss\n",
    "ffnn_loss_file_name = 'ffnn_bow_class_big_loss_500_epoch_100_less_lr.csv'\n",
    "f = open(ffnn_loss_file_name,'w')\n",
    "f.write('iter, loss')\n",
    "f.write('\\n')\n",
    "losses = []\n",
    "iter = 0\n",
    "# Start training\n",
    "for epoch in range(num_epochs):\n",
    "    if (epoch+1) % 25 == 0:\n",
    "        print(\"Epoch completed: \" + str(epoch+1))\n",
    "    train_loss = 0\n",
    "    for index, row in X_train.iterrows():\n",
    "        # Clearing the accumulated gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # TODO: Make the bag of words vector for stemmed tokens \n",
    "        bow_vec = None # Replace None with the correct code\n",
    "       \n",
    "        # TODO: Forward pass to get output\n",
    "        probs = None # Replace None with the correct code\n",
    "\n",
    "        # Get the target label\n",
    "        target = make_target(Y_train['sentiment'][index])\n",
    "\n",
    "        # TODO: Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = None # Replace None with the correct code\n",
    "        \n",
    "        # Accumulating the loss over time\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "    f.write(str((epoch+1)) + \",\" + str(train_loss / len(X_train)))\n",
    "    f.write('\\n')\n",
    "    train_loss = 0\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "blue {\n",
    "  color: skyblue;\n",
    "}\n",
    "\n",
    "green {\n",
    "  color: lightgreen;\n",
    "}\n",
    "</style>\n",
    "\n",
    "### **Step - 12**\n",
    "This code evaluates the performance of the trained feedforward neural network on the test dataset and visualizes the loss during training.\n",
    "\n",
    "1) <blue>**Prediction Loop**</blue>: It iterates through the test set X_test without tracking gradients <green>**(using torch.no_grad())**</green> for efficient inference. For each test sample, it generates a <green>**BOW vector**</green> and passes it through the model to obtain predicted probabilities.\n",
    "\n",
    "2) <blue>**Classification Report**</blue>: The <green>**classification_report**</green> from <green>**sklearn.metrics**</green> is printed to evaluate the model's performance, displaying precision, recall, F1-score, and support for each class.\n",
    "\n",
    "3) <blue>**Loss DataFrame**</blue>: The loss data saved in <green>**ffnn_loss_file_name**</green> is read into a DataFrame.\n",
    "\n",
    "4) <blue>**Loss Plotting**</blue>: The loss is plotted using pandas and saved as a JPEG file named <green>**\"ffnn_bow_loss_500_padding_100_epochs_less_lr.jpg\"**</green>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_ff_nn_predictions = []\n",
    "original_lables_ff_bow = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for index, row in X_test.iterrows():\n",
    "        bow_vec = make_bow_vector(review_dict, row['stemmed_tokens'])\n",
    "        probs = ff_nn_bow_model(bow_vec)\n",
    "        bow_ff_nn_predictions.append(torch.argmax(probs, dim=1).cpu().numpy()[0])\n",
    "        original_lables_ff_bow.append(make_target(Y_test['sentiment'][index]).cpu().numpy()[0])\n",
    "        \n",
    "print(classification_report(original_lables_ff_bow,bow_ff_nn_predictions))\n",
    "ffnn_loss_df = pd.read_csv(ffnn_loss_file_name)\n",
    "print(len(ffnn_loss_df))\n",
    "print(ffnn_loss_df.columns)\n",
    "ffnn_plt_500_padding_100_epochs = ffnn_loss_df[' loss'].plot()\n",
    "fig = ffnn_plt_500_padding_100_epochs.get_figure()\n",
    "fig.savefig(\"ffnn_bow_loss_500_padding_100_epochs_less_lr.jpg\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
