{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "===================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "blue {\n",
    "  color: skyblue;\n",
    "}\n",
    "</style>\n",
    "\n",
    "## 1) **Lowercasing**\n",
    "Lowercasing converts all characters in a text to <blue>**lowercase**</blue>. It ensures uniformity by treating words like <blue>**\"Dog\"**</blue> and <blue>**\"dog\"**</blue> as the same entity. This is important for many NLP tasks since capitalization usually doesn't change the meaning of words.\n",
    "\n",
    "Example:\\\n",
    "Input: \"Natural Language Processing\"\\\n",
    "Output: \"natural language processing\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world! welcome to our nlp - deep learning bootcamp\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello WorlD! Welcome to our NLP - Deep Learning Bootcamp\"\n",
    "lowercased_text = text.lower()\n",
    "\n",
    "print(lowercased_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "blue {\n",
    "  color: skyblue;\n",
    "}\n",
    "</style>\n",
    "## 2) **Removing Punctuation & Special Characters**\n",
    "\n",
    "Punctuation marks (like <blue>**commas**</blue>, <blue>**periods**</blue>, <blue>**dash**</blue> etc.) and special characters (like <blue>**@**</blue>, <blue>**#**</blue>, <blue>**$**</blue>, etc.) are often not meaningful in many NLP tasks. Removing them helps clean the text for better analysis.\n",
    "\n",
    "Example:\\\n",
    "Input: \"Hello! How are you doing @today?\"\\\n",
    "Output: \"Hello How are you doing today\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world  Welcome to our NLP  Deep Learning Bootcamp\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text = \"Hello, world âœ‹âœ‹! Welcome to?* our&/|~^+%'\\\" NLP - Deep LearningðŸ§  BootcampðŸ¤©.\"\n",
    "punctuation_pattern = r'[^\\w\\s]'\n",
    "text_cleaned = re.sub(punctuation_pattern, '', text)\n",
    "print(text_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "blue {\n",
    "  color: skyblue;\n",
    "}\n",
    "</style>\n",
    "## 3) **Stop - Words Removal**\n",
    "\n",
    "Stop-words are common words like <blue>**\"the\"**</blue>, <blue>**\"is\"**</blue>, <blue>**\"in\"**</blue>, <blue>**\"and\"**</blue> that don't contribute significant meaning to the text. Removing them helps reduce the size of the dataset <blue>**without losing important context**</blue>.\n",
    "\n",
    "Example:\\\n",
    "Input: \"This is a sample sentence\"\\\n",
    "Output: \"sample sentence\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language: english\n",
      "Filtered Text: ['Hello', 'World!', 'This', 'NLP', '-', 'Deep', 'Learning', 'Bootcamp.', 'Hope', 'fun!']\n",
      "Language: hinglish\n",
      "Filtered Text: ['Yeh', 'din', 'I', 'feeling', 'awesome']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Remove stopwords function for any language\n",
    "def remove_stopwords(text, language):\n",
    "    stop_words = set(stopwords.words(language))\n",
    "    word_tokens = text.split()\n",
    "    filtered_text = [word for word in word_tokens if word not in stop_words]\n",
    "    print(f\"Language: {language}\")\n",
    "    print(\"Filtered Text:\", filtered_text)\n",
    "\n",
    "# English Example\n",
    "en_text = \"Hello World! This is an NLP - Deep Learning Bootcamp. Hope this is fun!\"\n",
    "remove_stopwords(en_text, \"english\")\n",
    "\n",
    "# Hindi + English - Example\n",
    "hi_text = \"Yeh ek bahut accha din hai and I am feeling awesome\"\n",
    "remove_stopwords(hi_text, \"hinglish\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "blue {\n",
    "  color: skyblue;\n",
    "}\n",
    "</style>\n",
    "## 4) **Removal of URLs**\n",
    "\n",
    "URLs are often <blue>**irrelevant**</blue> in NLP tasks and can add noise to the data. Removing them ensures cleaner text without <blue>**web links**</blue> that donâ€™t contribute to the context.\n",
    "\n",
    "Example:\\\n",
    "Input: \"Check out this link: https://example.com\"\\\n",
    "Output: \"Check out this link\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I hope this bootcamp is useful for you. You can share it with your friends at '"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_urls(text):\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url_pattern.sub(r'', text)\n",
    "\n",
    "text = \"I hope this bootcamp is useful for you. You can share it with your friends at https://example.com\"\n",
    "remove_urls(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "blue {\n",
    "  color: skyblue;\n",
    "}\n",
    "</style>\n",
    "## 5) **Removal of HTML Tags**\n",
    "\n",
    "HTML tags are used in web data but are <blue>**unnecessary**</blue> in NLP tasks. <blue>**Stripping**</blue> out HTML tags cleans the text extracted from <blue>**web pages**</blue>.\n",
    "\n",
    "Example:\\\n",
    "Input: \"&lt;p>This is a paragraph.&lt;/p>\"\\\n",
    "Output: \"This is a paragraph.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NLP - Deep Learning\n",
      "Removal of HTML tags\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"\"\"<html><div>\n",
    "<h1>NLP - Deep Learning</h1>\n",
    "<p>Removal of HTML tags</p>\n",
    "<a href=\"https://example.com\"></a>\n",
    "</div></html>\"\"\"\n",
    "\n",
    "html_tags_pattern = r'<.*?>'\n",
    "text_without_html_tags = re.sub(html_tags_pattern, '', text)\n",
    "print(text_without_html_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "blue {\n",
    "  color: skyblue;\n",
    "}\n",
    "</style>\n",
    "## 6) **Stemming**\n",
    "\n",
    "Stemming reduces a word to its <blue>**base**</blue> or <blue>**root**</blue> form, which might not always be a valid word. The idea is to <blue>**strip**</blue> off <blue>**prefixes**</blue> or <blue>**suffixes**</blue>. Itâ€™s a quick and less computationally expensive way of normalizing words. Stemming is preferred when the <blue>**meaning**</blue> of the word is <blue>**not important**</blue> for analysis. for example: <blue>**Spam Detection**</blue>\n",
    "\n",
    "Example:\\\n",
    "Input: \"Playing\", \"Played\", \"Plays\"\\\n",
    "Output: \"Play\"\n",
    "\n",
    "<blue>**Porter stemming**</blue> algorithm is one of the most common stemming algorithms which is basically designed to <blue>**remove**</blue> and <blue>**replace**</blue> well-known <blue>**suffixes**</blue> of English words. Although the Porter Stemming Algorithm was developed for English texts, it can be adapted to different languages. However, it is more effective to use natural language processing tools and algorithms specifically designed for different languages, like the library <blue>**iNLTK**</blue> offers these tools for <blue>**Indic Languages**</blue>. You can find it out here: <blue>**https://github.com/goru001/inltk**</blue> \n",
    "\n",
    "<div style=\"font-style: italic; text-align: center;\" markdown=\"1\">\n",
    "<img width=\"30%\" src=\"https://cdn.botpenguin.com/assets/website/Stemming_53678d43bc.png\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text', 'preprocess', 'section', 'in', 'cours', 'nlp', '-', 'deep', 'learn']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    " \n",
    "def stem_words(text):\n",
    "    word_tokens = text.split()\n",
    "    stems = [stemmer.stem(word) for word in word_tokens]\n",
    "    return stems\n",
    " \n",
    "text = 'text preprocessing section in course nlp - deep learning'\n",
    "stem_words(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "blue {\n",
    "  color: skyblue;\n",
    "}\n",
    "</style>\n",
    "## 7) **Lemmatization**\n",
    "\n",
    "Lemmatization is a more advanced technique compared to stemming. It <blue>**reduces**</blue> a word to its <blue>**base form (called a lemma)**</blue> while ensuring the <blue>**output**</blue> is a <blue>**valid word**</blue>. It uses context to determine whether the word is in singular, plural, or tense forms.\n",
    "\n",
    "Example:\\\n",
    "Input: \"Running\", \"Ran\"\\\n",
    "Output: \"Run\"\n",
    "\n",
    "In our lemmatization example, we will be using a popular lemmatizer called <blue>**WordNet**</blue> lemmatizer. WordNet is a word association database for English and a useful resource for English lemmatization. A popular lemmatizer used for Hindi is developed by <blue>**JohSnowLabs**</blue> can be found here: <blue>**https://sparknlp.org/2020/07/29/lemma_hi.html**</blue>\n",
    "\n",
    "<div style=\"font-style: italic; text-align: center;\" markdown=\"1\">\n",
    "<img width=\"30%\" src=\"https://cdn.botpenguin.com/assets/website/Lemmatization_5338fc7c3e.png\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['text', 'preprocessing', 'section', 'in', 'course', 'nlp', '-', 'deep', 'learn']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_word(text):\n",
    "    word_tokens = text.split()\n",
    "    lemmas = [lemmatizer.lemmatize(word, pos ='v') for word in word_tokens]\n",
    "    return lemmas\n",
    " \n",
    "text = 'text preprocessing section in course nlp - deep learning'\n",
    "print(lemmatize_word(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "blue {\n",
    "  color: skyblue;\n",
    "}\n",
    "</style>\n",
    "## 8) **Tokenization**\n",
    "\n",
    "Tokenization is the process of <blue>**splitting**</blue> a text into <blue>**individual units**</blue> like words, phrases, or sentences, called <blue>**tokens**</blue>. These tokens form the building blocks for further processing and analysis in NLP tasks.\n",
    "\n",
    "Example:\\\n",
    "Input: \"Congratulations you are almost at the end of this file.\"\\\n",
    "Output: [\"Congratulations\", \"you\", \"are\", \"almost\", \"at\", \"the\", \"end\", \"of\", \"this\", \"file\", \".\"]\n",
    "\n",
    "There are different methods and libraries available to perform tokenization. <blue>**SpaCy**</blue> and <blue>**Gensim**</blue> are some of the libraries that can be used to accomplish the task.\n",
    "Tokenization can be used to separate words or sentences. If the text is split into <blue>**words**</blue> using some separation technique it is called <blue>**word tokenization**</blue> and the same separation done for <blue>**sentences**</blue> is called <blue>**sentence tokenization**</blue>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Congratulations', 'you', 'are', 'almost', 'at', 'the', 'end', 'of', 'this', 'file', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"Congratulations you are almost at the end of this file.\"\n",
    "\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
